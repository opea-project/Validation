# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

name: Run accuary test on a Gaudi

on:
  workflow_dispatch:
    inputs:
      examples:
        default: "ChatQnA"
        description: 'List of examples to test [AudioQnA,ChatQnA,CodeGen,FaqGen]'
        required: true
        type: string
      datasets:
        default: "en"
        description: "List of services to test [en, zh]"
        required: true
        type: string

  pull_request:
    branches: [main]
    types: [opened, reopened, ready_for_review, synchronize]

# If there is a new commit, the previous jobs will be canceled
concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  get-test-matrix:
    runs-on: ubuntu-latest
    outputs:
      datasets: ${{ steps.get-matrix.outputs.datasets }}
      examples: ${{ steps.get-matrix.outputs.examples }}
    steps:
      - name: Create Matrix
        id: get-matrix
        run: |
          # datasets=($(echo ${{ inputs.datasets }} | tr ',' ' '))
          datasets=($(echo "en" | tr ',' ' '))
          datasets_json=$(printf '%s\n' "${datasets[@]}" | jq -R '.' | jq -sc '.')
          echo "datasets=$datasets_json" >> $GITHUB_OUTPUT
          examples=($(echo "CodeGen" | tr ',' ' '))  
          examples_json=$(printf '%s\n' "${examples[@]}" | jq -R '.' | jq -sc '.')
          echo "examples=$examples_json" >> $GITHUB_OUTPUT

  setup-acc:
    runs-on: aise-acc
    needs: [get-test-matrix]
    strategy:
      matrix:
        dataset: ${{ fromJson(needs.get-test-matrix.outputs.datasets) }}
        example: ${{ fromJson(needs.get-test-matrix.outputs.examples) }}
        exclude:
          - example: [AudioQnA, CodeGen, FaqGen]
            dataset: zh
      fail-fast: false
    steps:
      - name: Clean up Working Directory
        run: |
          sudo rm -rf ${{github.workspace}}/* || true
          docker system prune -f
          docker rmi $(docker images --filter reference="100.83.111.229:5000/opea/*:latest" -q) || true

      - name: Checkout out Validation
        uses: actions/checkout@v4
        with:
          path: Validation

      - name: Checkout out GenAIEval
        uses: actions/checkout@v4
        with:
          repository: opea-project/GenAIEval
          path: GenAIEval

      - name: Checkout out GenAIExamples
        uses: actions/checkout@v4
        with:
          repository: opea-project/GenAIExamples
          path: GenAIExamples

      - name: Set up environment
        env:
          example: ${{ matrix.example }}
        run: |
          set -x
          conda_env_name="OPEA_acc"
          export PATH=${HOME}/miniforge3/bin/:$PATH
          if conda info --envs | grep -q "$conda_env_name"; then
            echo "$conda_env_name exist! Recreating..."
            conda env remove --name "$conda_env_name"
            conda create -n ${conda_env_name} python=3.12 -y
            echo "Created!"
          else
            conda create -n ${conda_env_name} python=3.12 -y
          fi
          source activate ${conda_env_name}
          echo "Dataset: ${{ matrix.dataset }}"
          echo "Example: ${{ matrix.example }}"
          cd ${{ github.workspace }}/GenAIEval
          pip install -r requirements.txt
          if [[ "${example}" == "AudioQnA" ]]; then
            cd ${{ github.workspace }}/GenAIExamples/AudioQnA/benchmark/accuracy
            pip install -r requirements.txt
          elif [[ "${example}" == "CodeGen" ]]; then
            pip install -e .
          fi

      - name: Launch Service
        env:
          HUGGINGFACEHUB_API_TOKEN: ${{ secrets.HUGGINGFACEHUB_API_TOKEN }}
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          example: ${{ matrix.example }}
          dataset: ${{ matrix.dataset }}
          IMAGE_REPO: 100.83.111.229:5000/opea
        run: |
          [ ! -d "acc-log" ] && mkdir acc-log
          cp ${{ github.workspace }}/Validation/.github/scripts/acc_test.sh ${{ github.workspace }}/GenAIExamples/${example}/benchmark/accuracy/
          # cd ${{ github.workspace }}/GenAIExamples/${example}/benchmark/accuracy/
          ls
          bash ${{ github.workspace }}/GenAIExamples/${example}/benchmark/accuracy/acc_test.sh --launch_service "${example}" "${dataset}" > ${{ github.workspace }}/acc-log/${example}-${dataset}-launch_service.txt
          # cd ${{ github.workspace }}/GenAIExamples/${example}/tests/
          # if [[ "${example}" == "ChatQnA" ]]; then
          #   if [[ "${dataset}" == "zh" ]]; then
          #     head -n "$(($(wc -l < test_compose_on_gaudi.sh) - 14))" "test_compose_on_gaudi.sh" > launch_"${example}"_"${dataset}".sh
          #     echo "    validate_megaservice" >> launch_"${example}"_"${dataset}".sh
          #     echo "}" >> launch_"${example}"_"${dataset}".sh
          #     echo "main" >> launch_"${example}"_"${dataset}".sh
          #     sed -i 's/export EMBEDDING_MODEL_ID="BAAI\/bge-base-en-v1.5"/export EMBEDDING_MODEL_ID="BAAI\/bge-base-zh-v1.5"/' launch_"${example}"_"${dataset}".sh
          #     sed -i 's/export LLM_MODEL_ID="Intel\/neural-chat-7b-v3-3"/export LLM_MODEL_ID="Qwen\/Qwen2-7B-Instruct"/' launch_"${example}"_"${dataset}".sh
          #     cat launch_"${example}"_"${dataset}".sh
          #     bash launch_"${example}"_"${dataset}".sh
          #   else
          #     head -n "$(($(wc -l < test_compose_on_gaudi.sh) - 14))" "test_compose_on_gaudi.sh" > launch_"${example}"_"${dataset}".sh
          #     echo "    validate_megaservice" >> launch_"${example}"_"${dataset}".sh
          #     echo "}" >> launch_"${example}"_"${dataset}".sh
          #     echo "main" >> launch_"${example}"_"${dataset}".sh
          #     cat launch_"${example}"_"${dataset}".sh
          #     bash launch_"${example}"_"${dataset}".sh
          #   fi
          # else
          #   if [[ "${example}" == "CodeGen" ]]; then sed -i 's/--max-total-tokens 2048/--max-total-tokens 4096/g' ${{ github.workspace }}/GenAIExamples/${example}/docker_compose/intel/hpu/gaudi/compose.yaml; fi
          #   head -n "$(($(wc -l < test_compose_on_gaudi.sh) - 10))" "test_compose_on_gaudi.sh" > launch_"${example}".sh
          #   echo "    validate_megaservice" >> launch_"${example}".sh
          #   echo "}" >> launch_"${example}".sh
          #   echo "main" >> launch_"${example}".sh
          #   cat launch_"${example}".sh
          #   bash launch_"${example}".sh
          # fi

      - name: Evaluation Prepare
        shell: bash
        if: ${{ matrix.dataset }} == 'en' && ${{ matrix.dataset }} == 'ChatQnA'
        env:
          HUGGINGFACEHUB_API_TOKEN: ${{ secrets.HUGGINGFACEHUB_API_TOKEN }}
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          example: ${{ matrix.example }}
          dataset: ${{ matrix.dataset }}
        run: |
          [ ! -d "acc-log" ] && mkdir acc-log
          cp ${{ github.workspace }}/Validation/.github/scripts/acc_test.sh ${{ github.workspace }}/GenAIExamples/${example}/benchmark/accuracy/
          # cd ${{ github.workspace }}/GenAIExamples/${example}/benchmark/accuracy/
          ls
          bash ${{ github.workspace }}/GenAIExamples/${example}/benchmark/accuracy/acc_test.sh --eval_prepare "${example}" "${dataset}" > ${{ github.workspace }}/acc-log/${example}-${dataset}-eval_prepare.txt

          # if [[ "${example}" == "ChatQnA" && "${dataset}" == "en" ]]; then
          #   cd ${{ github.workspace }}/GenAIEval/
          #   DPATH=$(dirname "$PWD")
          #   export PYTHONPATH=$PYTHONPATH:$DPATH
          #   export PATH=$PATH:/bin:/usr/bin
          #   cd ${{ github.workspace }}/GenAIEval/evals/evaluation/rag_eval/examples
          #   # docker run -tid -p 9001:80 --runtime=habana -e HABANA_VISIBLE_DEVICES=1,2 -e HABANA_VISIBLE_MODULES=6,7 -e
          #   docker run -tid -p 9001:80 --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e PT_HPU_ENABLE_LAZY_COLLECTIVES=true -e OMPI_MCA_btl_vader_single_copy_mechanism=none -e HF_TOKEN=${HF_TOKEN} --cap-add=sys_nice --ipc=host ghcr.io/huggingface/tgi-gaudi:2.0.1 --model-id mistralai/Mixtral-8x7B-Instruct-v0.1 --max-input-tokens 2048 --max-total-tokens 4096 --sharded true --num-shard 2
          # elif [[ "${example}" == "FaqGen" ]]; then
          #   export FAQ_ENDPOINT="http://${ip_address}:9000/v1/faqgen"
          #   cd ${{ github.workspace }}/GenAIExamples/${example}/benchmark/accuracy
          #   sed -i 's/f = open("data\/sqv2_context.json", "r")/f = open("\/data2\/opea-dataset\/FaqGen\/sqv2_context.json", "r")/g' generate_FAQ.py
          #   sed -i 's/f = open("data\/sqv2_context.json", "r")/f = open("\/data2\/opea-dataset\/FaqGen\/sqv2_context.json", "r")/g' evaluate.py
          #   sed -i 's/1024/7/g' post_process_FAQ.py
          #   [ ! -d "${{ github.workspace }}/GenAIExamples/${example}/benchmark/accuracy/data/result" ] && mkdir -p ${{ github.workspace }}/GenAIExamples/${example}/benchmark/accuracy/data/result
          #   python generate_FAQ.py
          #   python post_process_FAQ.py
          #   volume=$PWD/data
          #   model=meta-llama/Llama-2-7b-hf
          #   docker run -tid -p 8082:80 -v $volume:/data --runtime=habana -e HABANA_VISIBLE_DEVICES=all  -e PT_HPU_LAZY_MODE=0 -e OMPI_MCA_btl_vader_single_copy_mechanism=none  -e HF_TOKEN=$HF_TOKEN --cap-add=sys_nice --ipc=host  ghcr.io/huggingface/tgi-gaudi:2.0.5 --model-id $model --max-input-tokens 3072 --max-total-tokens 4096 
          #   # export LLM_ENDPOINT="http://${ip_address}:8082"
          #   # curl http://${ip_address}:8082/generate \
          #   #   -X POST \
          #   #   -d '{"inputs":"What is Deep Learning?","parameters":{"max_new_tokens":128}}' \
          #   #   -H 'Content-Type: application/json'
          # fi

      - name: Run Evaluation
        env:
          HUGGINGFACEHUB_API_TOKEN: ${{ secrets.HUGGINGFACEHUB_API_TOKEN }}
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          example: ${{ matrix.example }}
          dataset: ${{ matrix.dataset }}
          port: 9001
        run: |
          set -x
          [ ! -d "acc-log" ] && mkdir acc-log
          cp ${{ github.workspace }}/Validation/.github/scripts/acc_test.sh ${{ github.workspace }}/GenAIExamples/${example}/benchmark/accuracy/
          bash ${{ github.workspace }}/GenAIExamples/${example}/benchmark/accuracy/acc_test.sh --launch_acc "${example}" "${dataset}" > ${{ github.workspace }}/acc-log/${example}-${dataset}-acc_test.txt
          # cd ${{ github.workspace }}/GenAIEval/
          # DPATH=$(pwd)
          # export PYTHONPATH=$PYTHONPATH:$DPATH
          # export PATH=$PATH:/bin:/usr/bin
          # cd ${{ github.workspace }}/GenAIEval/evals/evaluation/rag_eval/examples
          # if [[ "${dataset}" == "en" ]]; then
          #  # --ragas_metrics 
          #   python eval_multihop.py --docs_path /data2/opea-dataset/${{ matrix.example }}/MultiHop-RAG/dataset/corpus.json  --dataset_path /data2/opea-dataset/${{ matrix.example }}/MultiHop-RAG/dataset/MultiHopRAG.json --ingest_docs --chunk_size 512 --chunk_overlap 200 --retrieval_metrics --llm_endpoint http://${ip_address}:${port}/generate > en-without-rerank.txt

          #   python eval_multihop.py --docs_path /data2/opea-dataset/${{ matrix.example }}/MultiHop-RAG/dataset/corpus.json  --dataset_path /data2/opea-dataset/${{ matrix.example }}/MultiHop-RAG/dataset/MultiHopRAG.json --retrieval_metrics --rerank --retrival_k 20 --llm_endpoint http://${ip_address}:${port}/generate > en-with-rerank.txt

          # elif [[ "${dataset}" == "zh" ]]; then
          #   python eval_crud.py --dataset_path /data2/opea-dataset/${{ matrix.example }}/data/split_merged.json --docs_path /data2/opea-dataset/${{ matrix.example }}/data/80000_docs --ingest_docs > zh.txt
          # fi

      - name: Clean up container
        if: always()
        env: 
          example: ${{ matrix.example }}
        run: |
          docker system prune -f
          cd ${{ github.workspace }}/GenAIExamples/${example}/docker_compose/intel/hpu/gaudi
          docker compose stop && docker compose rm -f
          docker rmi $(docker images --filter reference="100.83.111.229:5000/opea/*:latest" -q) || true

      - name: Publish pipeline artifact
        if: ${{ !cancelled() }}
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.example }}
          path: ${{ github.workspace }}/acc-log/*.txt
