# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

name: Run accuary test on a Gaudi

on:
  workflow_dispatch:
    inputs:
      # registry:
      #   default: ""
      #   description: "Registry to store images,e.g., docker.io, default is empty"
      #   required: false
      #   type: string
      # nodes:
      #   default: "gaudi"
      #   description: "Hardware to run test"
      #   required: true
      #   type: string
      datasets:
        default: "en"
        description: "List of services to test [en, zh]"
        required: true
        type: string
      # tag:
      #   default: "rc0.9"
      #   description: "Tag to apply to images"
      #   required: true
      #   type: string
      # opea_branch:
      #   default: "rc0.9"
      #   description: "Branch to build images"
      #   required: true
      #   type: string
      # runner:
      #   description: "Runner label"
      #   required: true
      #   type: string
  pull_request:
    branches: [main]
    types: [opened, reopened, ready_for_review, synchronize]

# If there is a new commit, the previous jobs will be canceled
concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  get-test-matrix:
    runs-on: ubuntu-latest
    outputs:
      datasets: ${{ steps.get-matrix.outputs.datasets }}
    steps:
      - name: Create Matrix
        id: get-matrix
        run: |
          # datasets=($(echo ${{ inputs.datasets }} | tr ',' ' '))
          datasets=($(echo "en,zh" | tr ',' ' '))
          datasets_json=$(printf '%s\n' "${datasets[@]}" | jq -R '.' | jq -sc '.')
          echo "datasets=$datasets_json" >> $GITHUB_OUTPUT

  setup-stress:
    runs-on: aise-perf
    needs: [get-test-matrix]
    strategy:
      matrix:
        dataset: ${{ fromJson(needs.get-test-matrix.outputs.datasets) }}
      fail-fast: false
    steps:
      - name: Clean Up Working Directory
        run: sudo rm -rf ${{github.workspace}}/*

      - name: Checkout out Validation
        uses: actions/checkout@v4
        with:
          path: Validation

      - name: Checkout out GenAIEval
        uses: actions/checkout@v4
        with:
          repository: opea-project/GenAIEval
          path: GenAIEval

      - name: Checkout out GenAIExamples
        uses: actions/checkout@v4
        with:
          repository: opea-project/GenAIExamples
          path: GenAIExamples

      - name: Set up environment
        run: |
          cd GenAIEval
          pip install -r requirements.txt

      - name: Prepare Dataset
        id: check-dataset
        env:
          dataset: ${{ matrix.dataset }}
        run: |
          cd ${{ github.workspace }}/GenAIEval/evals/evaluation/rag_eval/
          if [[ "${dataset}" == "en" ]]; then
            cd examples
            git clone https://github.com/yixuantt/MultiHop-RAG.git
            need_launch=true
            echo "need_launch=$need_launch" >>$GITHUB_OUTPUT
          elif [[ "${dataset}" == "zh" ]]; then
            git clone https://github.com/IAAR-Shanghai/CRUD_RAG
            mkdir data/
            cp CRUD_RAG/data/crud_split/split_merged.json data/
            cp -r CRUD_RAG/data/80000_docs/ data/
            python examples/process_crud_dataset.py
          fi

      - name: Launch Service of RAG System
        env:
          dataset: ${{ matrix.dataset }}
          node: ${{ matrix.node }}
        run: |
          export IMAGE_TAG=acc
          cd ${{ github.workspace }}/GenAIExamples/ChatQnA/tests/
          if [[ "${dataset}" == "zh" ]]; then
            sed -i 's/export EMBEDDING_MODEL_ID="BAAI\/bge-base-en-v1.5"/export EMBEDDING_MODEL_ID="BAAI\/bge-base-zh-v1.5"/' test_compose_on_gaudi.sh
            sed -i 's/export LLM_MODEL_ID="Intel\/neural-chat-7b-v3-3"/export LLM_MODEL_ID="Qwen\/Qwen2-7B-Instruct"/' test_compose_on_gaudi.sh
          fi
          head -n "$(($(wc -l < test_compose_on_gaudi.sh) - 10))" "test_compose_on_gaudi.sh" > temp_file.sh
          echo "}" >> temp_file.sh
          echo "main" >> temp_file.sh
          cat temp_file.sh
          bash temp_file.sh

      - name: Launch Service of LLM-as-a-Judge
        shell: bash
        if: steps.check-dataset.outputs.need_launch == 'true'
        env:
          HUGGINGFACEHUB_API_TOKEN: ${{ secrets.HUGGINGFACEHUB_API_TOKEN }}
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          dataset: ${{ matrix.dataset }}
          port: 9001
        run: |
          cd ${{ github.workspace }}/GenAIEval/
          PATH=$(dirname "$PWD")
          export PYTHONPATH=$PYTHONPATH:$PATH
          cd ${{ github.workspace }}/GenAIEval/evals/evaluation/rag_eval/examples
          # docker run -tid -p ${port}:80 --runtime=habana -e HABANA_VISIBLE_DEVICES=1,2 -e HABANA_VISIBLE_MODULES=6,7 -e
          docker run -tid -p ${port}:80 --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e PT_HPU_ENABLE_LAZY_COLLECTIVES=true -e OMPI_MCA_btl_vader_single_copy_mechanism=none -e HF_TOKEN=${HF_TOKEN} --cap-add=sys_nice --ipc=host ghcr.io/huggingface/tgi-gaudi:2.0.1 --model-id mistralai/Mixtral-8x7B-Instruct-v0.1 --max-input-tokens 2048 --max-total-tokens 4096 --sharded true --num-shard 2

      - name: Run Evaluation
        env:
          HUGGINGFACEHUB_API_TOKEN: ${{ secrets.HUGGINGFACEHUB_API_TOKEN }}
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          dataset: ${{ matrix.dataset }}
        run: |
          cd ${{ github.workspace }}/evals/evaluation/rag_eval/examples
          if [[ "${dataset}" == "en" ]]; then
            python eval_multihop.py --docs_path MultiHop-RAG/dataset/corpus.json  --dataset_path MultiHop-RAG/dataset/MultiHopRAG.json --ingest_docs --retrieval_metrics --ragas_metrics --llm_endpoint http://{your_ip}:{your_llm_port}/generate
          elif [[ "${dataset}" == "zh" ]]; then
            python eval_crud.py --dataset_path ../data/split_merged.json --docs_path ../data/80000_docs --ingest_docs
          fi
